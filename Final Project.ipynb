{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b39092fe-0db9-421a-ad5a-4c010e628f29",
   "metadata": {},
   "source": [
    "# Final Project | Cody Laurie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d6e16a-31e1-4842-9b59-830895e62015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np   # Needed for np.where\n",
    "\n",
    "# Load CAASPP dataset (caret-delimited \"^\") with latin1 encoding\n",
    "caaspp = pd.read_csv(\n",
    "    \"sb_ca2024_all_csv_v1.txt\",\n",
    "    delimiter=\"^\",\n",
    "    encoding=\"latin1\",\n",
    "    dtype={\n",
    "        \"County Code\": str,\n",
    "        \"District Code\": str,\n",
    "        \"School Code\": str\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create CDSCode (County + District + School) â†’ 14-digit string\n",
    "caaspp[\"CDSCode\"] = (\n",
    "    caaspp[\"County Code\"].str.zfill(2) +\n",
    "    caaspp[\"District Code\"].str.zfill(5) +\n",
    "    caaspp[\"School Code\"].str.zfill(7)\n",
    ")\n",
    "\n",
    "# Load DF dataset (budget/expenditure)\n",
    "df = pd.read_csv(\"df_3_4_inner.csv\")\n",
    "\n",
    "# Drop unnamed columns if they exist\n",
    "df = df.loc[:, ~df.columns.str.contains(r\"^Unnamed\")]\n",
    "\n",
    "# Ensure CDSCODE loads as string and has correct padding\n",
    "df[\"CDSCODE\"] = df[\"CDSCODE\"].astype(str).str.zfill(14)\n",
    "\n",
    "# Ensure ZIP is numeric\n",
    "df[\"ZIP\"] = pd.to_numeric(df[\"ZIP\"], errors=\"coerce\")\n",
    "\n",
    "# Ensure budget fields are numeric\n",
    "df[\"Budget (incl c/o) FY24\"] = pd.to_numeric(df[\"Budget (incl c/o) FY24\"], errors=\"coerce\")\n",
    "df[\"Expenditures FY24\"] = pd.to_numeric(df[\"Expenditures FY24\"], errors=\"coerce\")\n",
    "df[\"% Exp FY24\"] = pd.to_numeric(df[\"% Exp FY24\"], errors=\"coerce\")\n",
    "\n",
    "# Merge datasets (intersection)\n",
    "merged = df.merge(\n",
    "    caaspp,\n",
    "    left_on=\"CDSCODE\",\n",
    "    right_on=\"CDSCode\",\n",
    "    how=\"inner\",\n",
    "    suffixes=(\"_budget\", \"_caaspp\")\n",
    ")\n",
    "\n",
    "\n",
    "# Convert CAASPP testing columns to numeric (required!)\n",
    "for col in [\"Total Students Tested\", \"Total Students Enrolled\"]:\n",
    "    if col in merged.columns:\n",
    "        merged[col] = pd.to_numeric(merged[col], errors=\"coerce\")\n",
    "    else:\n",
    "        print(f\"WARNING: Column '{col}' not found in CAASPP file.\")\n",
    "\n",
    "# Add Participation Rate (%) safely depricated but kept for safety\n",
    "merged[\"Participation Rate (%)\"] = np.where(\n",
    "    (merged[\"Total Students Enrolled\"] > 0) & (~merged[\"Total Students Enrolled\"].isna()),\n",
    "    (merged[\"Total Students Tested\"] / merged[\"Total Students Enrolled\"]) * 100,\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# Quick sanity checks\n",
    "print(\"Original DF (budget) shape:\", df.shape)\n",
    "print(\"CAASPP shape:\", caaspp.shape)\n",
    "print(\"Merged (intersection) shape:\", merged.shape)\n",
    "\n",
    "print(\"\\nMerged head (showing Participation Rate):\")\n",
    "cols_to_show = [\n",
    "    \"CDSCODE\",\n",
    "    \"School Name\" if \"School Name\" in merged.columns else None,\n",
    "    \"Total Students Tested\",\n",
    "    \"Total Students Enrolled\",\n",
    "    \"Participation Rate (%)\"\n",
    "]\n",
    "cols_to_show = [c for c in cols_to_show if c in merged.columns]\n",
    "\n",
    "print(merged[cols_to_show].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a224c894-9585-4a25-9509-e42a1093b70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter merged CAASPP rows: All Students + ELA\n",
    "# Student Group ID = 1 -> All Students\n",
    "# Test ID = 1 -> ELA\n",
    "\n",
    "filtered = merged[\n",
    "    (merged[\"Student Group ID\"] == 1) &\n",
    "    (merged[\"Test ID\"] == 1)\n",
    "].copy()\n",
    "\n",
    "print(\"Filtered CAASPP rows (All Students + ELA):\", filtered.shape)\n",
    "filtered[[\n",
    "    \"CDSCODE\",\n",
    "    \"Student Group ID\",\n",
    "    \"Test ID\",\n",
    "    \"Mean Scale Score\",\n",
    "    \"Percentage Standard Met and Above\",\n",
    "    \"Participation Rate (%)\"\n",
    "]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526a2922-9fbf-4e95-8bc4-8bbd3705415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the raw values in the problematic column\n",
    "\n",
    "print(\"Example raw values in 'Percentage Standard Met and Above':\")\n",
    "print(filtered[\"Percentage Standard Met and Above\"].head(20).tolist())\n",
    "\n",
    "# Detect any values that contain characters other than digits, dot, or minus\n",
    "weird_mask = filtered[\"Percentage Standard Met and Above\"].astype(str).str.contains(\n",
    "    r\"[^0-9.\\-]\", regex=True\n",
    ")\n",
    "\n",
    "print(\"\\nValues that contain '*' or other non-numeric characters:\")\n",
    "print(filtered.loc[weird_mask, \"Percentage Standard Met and Above\"].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bf442f-f739-43b2-9ab1-4a5b9fa86b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert score columns to numeric, invalid entries -> NaN\n",
    "\n",
    "cols_to_convert = [\"Percentage Standard Met and Above\", \"Mean Scale Score\"]\n",
    "\n",
    "for col in cols_to_convert:\n",
    "    filtered[col] = pd.to_numeric(filtered[col], errors=\"coerce\")\n",
    "\n",
    "print(filtered[cols_to_convert].dtypes)\n",
    "\n",
    "print(\"\\nSummary after conversion:\")\n",
    "print(filtered[cols_to_convert].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cc3cdc-4fa7-444d-a075-680ef059318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate to one performance row per school (CDSCODE)\n",
    "\n",
    "school_perf = filtered.groupby(\"CDSCODE\", as_index=False).agg({\n",
    "    \"Percentage Standard Met and Above\": \"mean\",\n",
    "    \"Mean Scale Score\": \"mean\"\n",
    "})\n",
    "\n",
    "school_perf.rename(columns={\n",
    "    \"Percentage Standard Met and Above\": \"Pct_Met_Above\",\n",
    "    \"Mean Scale Score\": \"Mean_Score\"\n",
    "}, inplace=True)\n",
    "\n",
    "print(\"School-level performance shape:\", school_perf.shape)\n",
    "print(school_perf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cbfb06-0454-4d02-be38-7c8bc4520b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge performance back with the budget/expenditure DF\n",
    "\n",
    "final = df.merge(school_perf, on=\"CDSCODE\", how=\"inner\")\n",
    "\n",
    "print(\"\\nFinal modeling dataset shape (funding + scores):\", final.shape)\n",
    "print(final[[\n",
    "    \"MPD_NAME\", \"CDSCODE\",\n",
    "    \"Budget (incl c/o) FY24\", \"Expenditures FY24\", \"% Exp FY24\",\n",
    "    \"Pct_Met_Above\", \"Mean_Score\"\n",
    "]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bee5c52-4322-41e0-bbb4-b2a890d165dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- 1. Add school-level Participation Rate (%) into `final` if present in `merged` ---\n",
    "if (\"Participation Rate (%)\" in merged.columns) and (\"Participation Rate (%)\" not in final.columns):\n",
    "    part_by_school = (\n",
    "        merged.groupby(\"CDSCODE\", as_index=False)[\"Participation Rate (%)\"]\n",
    "              .mean()   # Average participation across grades\n",
    "    )\n",
    "    final = final.merge(part_by_school, on=\"CDSCODE\", how=\"left\")\n",
    "\n",
    "\n",
    "# --- 2. Convert numeric columns to proper dtypes ---\n",
    "numeric_cols = [\n",
    "    \"Budget (incl c/o) FY24\",\n",
    "    \"Expenditures FY24\",\n",
    "    \"% Exp FY24\",\n",
    "    \"Pct_Met_Above\",\n",
    "    \"Mean_Score\",\n",
    "    \"Participation Rate (%)\",\n",
    "]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    final[col] = pd.to_numeric(final[col], errors=\"coerce\")\n",
    "\n",
    "final_clean = final.dropna(subset=numeric_cols).copy()\n",
    "print(\"After numeric conversion and dropna:\", final_clean.shape)\n",
    "print(\"Columns in final_clean:\\n\", final_clean.columns.tolist())\n",
    "\n",
    "\n",
    "# --- 3. Correlation matrix ---\n",
    "corr_matrix = final_clean[numeric_cols].corr()\n",
    "print(\"\\nCorrelation matrix (funding vs scores + participation):\")\n",
    "print(corr_matrix)\n",
    "\n",
    "\n",
    "# --- 4. Lift between high-funded and low-funded schools ---\n",
    "median_exp = final_clean[\"Expenditures FY24\"].median()\n",
    "print(\"\\nMedian Expenditures FY24:\", median_exp)\n",
    "\n",
    "final_clean[\"Funding_Level\"] = np.where(\n",
    "    final_clean[\"Expenditures FY24\"] >= median_exp,\n",
    "    \"High\",\n",
    "    \"Low\"\n",
    ")\n",
    "\n",
    "lift_table = final_clean.groupby(\"Funding_Level\")[\"Pct_Met_Above\"].mean()\n",
    "high_mean = lift_table.get(\"High\", np.nan)\n",
    "low_mean = lift_table.get(\"Low\", np.nan)\n",
    "lift_value = high_mean / low_mean if (low_mean not in [0, np.nan]) else np.nan\n",
    "\n",
    "print(\"\\nAverage % Met & Above by Funding Level:\")\n",
    "print(lift_table)\n",
    "\n",
    "print(\"\\nLift (High funding vs Low funding) on % Met & Above:\")\n",
    "print(\"Lift =\", lift_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633109a4-6fd2-4505-806f-a987539e140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296e62ab-909f-4b39-aa2f-97a450984f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select approved features\n",
    "\n",
    "numeric_features = [\n",
    "    \"Budget (incl c/o) FY24\",\n",
    "    \"Expenditures FY24\",\n",
    "    \"% Exp FY24\",\n",
    "    \"ZIP\",\n",
    "    \"Participation Rate (%)\"   # <-- added, but no Grade\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"MPD_TYPE\",\n",
    "    \"MAP_TYPE\",\n",
    "    \"CHARTER\",\n",
    "    \"LD\"\n",
    "]\n",
    "\n",
    "# Extract numeric features\n",
    "X_numeric = final_clean[numeric_features].copy()\n",
    "\n",
    "# One-hot encode categorical features\n",
    "X_categorical = pd.get_dummies(\n",
    "    final_clean[categorical_features],\n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "# Combine into full feature matrix\n",
    "X = pd.concat([X_numeric, X_categorical], axis=1)\n",
    "\n",
    "# Create binary target variable (High vs Low performance)\n",
    "y = (final_clean[\"Pct_Met_Above\"] >= final_clean[\"Pct_Met_Above\"].median()).astype(int)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y distribution:\\n\", y.value_counts())\n",
    "print(\"\\nPreview of X:\")\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac15fa0e-d716-49d6-89bc-da59db5ebff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Train/test split (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale numeric features for Logistic Regression\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c1ad4c-0b51-4f6d-b08d-d0f789909bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train the model\n",
    "log_reg = LogisticRegression(max_iter=500)\n",
    "log_reg.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc968f1-5b1f-41fe-b82f-125e9994e412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80721bec-a748-453f-8d0d-43d693ea64fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Match coefficients to feature names\n",
    "coef_df = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Coefficient\": log_reg.coef_[0]\n",
    "}).sort_values(by=\"Coefficient\", ascending=False)\n",
    "\n",
    "coef_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187536a5-35e4-49ea-9841-9585c360f822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Same split as logistic regression for comparison\n",
    "X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677aa051-dc55-4ef8-ac86-fb5bd51f5eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,        \n",
    "    max_depth=None,         \n",
    "    random_state=42,\n",
    "    class_weight=\"balanced\" \n",
    ")\n",
    "\n",
    "rf.fit(X_train_rf, y_train_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6676fb92-7590-4833-b133-7de0e831e283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "y_pred_rf = rf.predict(X_test_rf)\n",
    "\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test_rf, y_pred_rf))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test_rf, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789a87cc-522b-4665-a3dc-32e624fb5e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "feature_importance_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d556a0-fd38-4218-83f9-bc73e856f602",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df.head(15).plot(kind='barh', x='Feature', figsize=(8,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec64ee7f-d47d-4f8b-9aa5-005e31070fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, classification_report, log_loss\n",
    ")\n",
    "from scipy.special import softmax\n",
    "\n",
    "TEST_SIZE = 0.20       # 20% test\n",
    "VAL_SIZE = 0.25        # 25% of (train+val) used as val\n",
    "N_ESTIMATORS = 200\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Composite metric weights: [accuracy, precision, recall, f1]\n",
    "COMPOSITE_WEIGHTS = np.array([1.0, 1.0, 1.0, 1.0])\n",
    "\n",
    "# Softmax temperature \n",
    "TEMPERATURE = 1.0\n",
    "\n",
    "\n",
    "# 1. Train / Val / Test split\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val,\n",
    "    test_size=VAL_SIZE, random_state=RANDOM_STATE, stratify=y_train_val\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n",
    "\n",
    "# 2. Train standard Random Forest\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=N_ESTIMATORS,\n",
    "    random_state=RANDOM_STATE,\n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Standard RF predictions (equal-weight voting)\n",
    "proba_std = rf.predict_proba(X_test)\n",
    "y_pred_std = np.argmax(proba_std, axis=1)\n",
    "logloss_std = log_loss(y_test, proba_std)\n",
    "\n",
    "print(\"\\n=== Standard Random Forest ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_std))\n",
    "print(\"Log-loss:\", logloss_std)\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred_std))\n",
    "\n",
    "# 3. Compute per-tree metrics on validation set\n",
    "tree_metrics = []   # rows: [acc, prec, rec, f1] per tree\n",
    "\n",
    "for tree in rf.estimators_:\n",
    "    val_pred = tree.predict(X_val)\n",
    "    acc = accuracy_score(y_val, val_pred)\n",
    "    prec = precision_score(y_val, val_pred, zero_division=0)\n",
    "    rec = recall_score(y_val, val_pred, zero_division=0)\n",
    "    f1 = f1_score(y_val, val_pred, zero_division=0)\n",
    "    tree_metrics.append([acc, prec, rec, f1])\n",
    "\n",
    "tree_metrics = np.array(tree_metrics)  # shape: (n_trees, 4)\n",
    "\n",
    "# Composite score = weighted sum of [acc, prec, rec, f1]\n",
    "composite_scores = tree_metrics.dot(COMPOSITE_WEIGHTS)\n",
    "\n",
    "# Softmax over trees -> normalized weights that sum to 1\n",
    "weights = softmax(composite_scores / TEMPERATURE)\n",
    "\n",
    "print(\"\\nFirst 5 composite scores:\", np.round(composite_scores[:5], 4))\n",
    "print(\"First 5 weights:\", np.round(weights[:5], 4))\n",
    "print(\"Sum of all weights (should be 1.0):\", weights.sum())\n",
    "\n",
    "# view tree metrics + weights\n",
    "tree_summary = pd.DataFrame(\n",
    "    tree_metrics,\n",
    "    columns=[\"acc\", \"prec\", \"rec\", \"f1\"]\n",
    ")\n",
    "tree_summary[\"composite\"] = composite_scores\n",
    "tree_summary[\"weight\"] = weights\n",
    "print(\"\\nPer-tree metric summary (head):\")\n",
    "print(tree_summary.head())\n",
    "\n",
    "# 4. Softmax-weighted ensemble on test set\n",
    "# Start with zeros, then add each tree's probabilities scaled by its weight\n",
    "proba_weighted = np.zeros_like(proba_std)\n",
    "\n",
    "for w, tree in zip(weights, rf.estimators_):\n",
    "    proba_weighted += w * tree.predict_proba(X_test)\n",
    "\n",
    "y_pred_weighted = np.argmax(proba_weighted, axis=1)\n",
    "logloss_weighted = log_loss(y_test, proba_weighted)\n",
    "\n",
    "print(\"\\n=== Softmax-Weighted Random Forest ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_weighted))\n",
    "print(\"Log-loss:\", logloss_weighted)\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3842032a-9092-43e7-8f74-2d8d4642f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuilding metrics_matrix, val_proba_per_tree, test_proba_per_tree\n",
    "\n",
    "metrics_matrix = []            # Will become shape (n_trees, 4)\n",
    "val_proba_per_tree = []        # list of arrays\n",
    "test_proba_per_tree = []       # list of arrays\n",
    "\n",
    "for tree in rf.estimators_:\n",
    "    # Per-tree predictions on validation set\n",
    "    val_pred = tree.predict(X_val.values)\n",
    "    acc = accuracy_score(y_val, val_pred)\n",
    "    prec = precision_score(y_val, val_pred, zero_division=0)\n",
    "    rec = recall_score(y_val, val_pred, zero_division=0)\n",
    "    f1 = f1_score(y_val, val_pred, zero_division=0)\n",
    "    metrics_matrix.append([acc, prec, rec, f1])\n",
    "\n",
    "    # Per-tree probabilities on val and test sets\n",
    "    val_proba_per_tree.append(tree.predict_proba(X_val.values))\n",
    "    test_proba_per_tree.append(tree.predict_proba(X_test.values))\n",
    "\n",
    "metrics_matrix = np.array(metrics_matrix)\n",
    "\n",
    "print(\"metrics_matrix shape:\", metrics_matrix.shape)\n",
    "print(\"example row:\", metrics_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eca78c-2c79-4526-a204-ac7158ca5460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRED SETUP BEFORE WEIGHT TUNING\n",
    "\n",
    "temperature = 1.0   # Default softmax temperature is 1 for reference\n",
    "\n",
    "metrics_matrix = []\n",
    "val_proba_per_tree = []\n",
    "test_proba_per_tree = []\n",
    "\n",
    "for tree in rf.estimators_:\n",
    "    # Per tree validation predictions\n",
    "    val_pred = tree.predict(X_val.values)\n",
    "    acc = accuracy_score(y_val, val_pred)\n",
    "    prec = precision_score(y_val, val_pred, zero_division=0)\n",
    "    rec = recall_score(y_val, val_pred, zero_division=0)\n",
    "    f1 = f1_score(y_val, val_pred, zero_division=0)\n",
    "    metrics_matrix.append([acc, prec, rec, f1])\n",
    "\n",
    "    # Per tree probabilities on val/test\n",
    "    val_proba_per_tree.append(tree.predict_proba(X_val.values))\n",
    "    test_proba_per_tree.append(tree.predict_proba(X_test.values))\n",
    "\n",
    "metrics_matrix = np.array(metrics_matrix)\n",
    "\n",
    "print(\"metrics_matrix shape:\", metrics_matrix.shape)\n",
    "print(\"first row of metrics_matrix:\", metrics_matrix[0])\n",
    "print(\"temperature set to:\", temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d398b3-0382-4fc3-86d8-a4d1b987cf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, log_loss, classification_report\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Pre compute per tree probabilities on val and test sets\n",
    "val_proba_per_tree = [tree.predict_proba(X_val.values) for tree in rf.estimators_]\n",
    "test_proba_per_tree = [tree.predict_proba(X_test.values) for tree in rf.estimators_]\n",
    "\n",
    "def evaluate_weight_vector(weight_vec, metrics_matrix,\n",
    "                           val_proba_per_tree, y_val, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Given a 4-dim weight vector [w_acc, w_prec, w_rec, w_f1],\n",
    "    compute composite scores -> softmax tree weights -> validation performance.\n",
    "    Returns (val_logloss, val_accuracy, tree_weights).\n",
    "    \"\"\"\n",
    "    weight_vec = np.array(weight_vec, dtype=float)\n",
    "    # Per tree composite score\n",
    "    composite_scores = metrics_matrix @ weight_vec\n",
    "    # Softmax to get non-negative weights that sum to 1\n",
    "    tree_weights = softmax(composite_scores / temperature)\n",
    "\n",
    "    # Aggregate weighted probabilities on validation set\n",
    "    proba_val = np.zeros_like(val_proba_per_tree[0])\n",
    "    for tw, p in zip(tree_weights, val_proba_per_tree):\n",
    "        proba_val += tw * p\n",
    "\n",
    "    val_logloss = log_loss(y_val, proba_val)\n",
    "    val_acc = accuracy_score(y_val, np.argmax(proba_val, axis=1))\n",
    "    return val_logloss, val_acc, tree_weights\n",
    "\n",
    "# Define a small grid of candidate weights\n",
    "candidate_vals = [0.0, 0.5, 1.0, 2.0]  # you can tweak/expand this later\n",
    "weight_grid = list(itertools.product(candidate_vals, repeat=4))\n",
    "print(f\"Evaluating {len(weight_grid)} weight combinations...\")\n",
    "\n",
    "best = {\n",
    "    \"w\": None,\n",
    "    \"val_logloss\": np.inf,\n",
    "    \"val_acc\": 0.0\n",
    "}\n",
    "\n",
    "for w in weight_grid:\n",
    "    ll, acc, _ = evaluate_weight_vector(\n",
    "        w, metrics_matrix, val_proba_per_tree, y_val, temperature=temperature\n",
    "    )\n",
    "    if ll < best[\"val_logloss\"]:\n",
    "        best[\"w\"] = w\n",
    "        best[\"val_logloss\"] = ll\n",
    "        best[\"val_acc\"] = acc\n",
    "\n",
    "print(\"\\nBest weight vector [acc, prec, rec, f1]:\", best[\"w\"])\n",
    "print(f\"Best validation log-loss: {best['val_logloss']:.4f}\")\n",
    "print(f\"Best validation accuracy: {best['val_acc']:.4f}\")\n",
    "\n",
    "# Use best weights to evaluate on TEST set\n",
    "best_w = np.array(best[\"w\"], dtype=float)\n",
    "best_composite_scores = metrics_matrix @ best_w\n",
    "best_tree_weights = softmax(best_composite_scores / temperature)\n",
    "\n",
    "# Aggregate weighted probabilities on test set\n",
    "proba_test = np.zeros_like(test_proba_per_tree[0])\n",
    "for tw, p in zip(best_tree_weights, test_proba_per_tree):\n",
    "    proba_test += tw * p\n",
    "\n",
    "test_logloss = log_loss(y_test, proba_test)\n",
    "y_pred_test = np.argmax(proba_test, axis=1)\n",
    "\n",
    "print(\"\\n=== Softmax RF with TUNED weights (Test set) ===\")\n",
    "print(f\"Test accuracy: {accuracy_score(y_test, y_pred_test):.4f}\")\n",
    "print(f\"Test log-loss: {test_logloss:.4f}\")\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331cf90b-b99b-400b-a644-dad65a3199dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the cleaned dataset with funding performance metadata\n",
    "final_clean.to_csv(\"school_funding_performance_clean.csv\", index=False)\n",
    "print(\"Exported as school_funding_performance_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cae3e3-099f-4248-9d7a-f80f704806ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the usable columns (assumed best fit by looking)\n",
    "usable_columns = [\n",
    "    \"CDSCODE\",\n",
    "    \"MPD_NAME\",\n",
    "    \"ZIP\",\n",
    "    \"MPD_TYPE\",\n",
    "    \"MAP_TYPE\",\n",
    "    \"CHARTER\",\n",
    "    \"LD\",\n",
    "    \"Campus\",\n",
    "    \"Budget (incl c/o) FY24\",\n",
    "    \"Expenditures FY24\",\n",
    "    \"% Exp FY24\",\n",
    "    \"Pct_Met_Above\",\n",
    "    \"Mean_Score\"\n",
    "]\n",
    "\n",
    "if \"Performance_Level\" in final_clean.columns:\n",
    "    usable_columns.append(\"Performance_Level\")\n",
    "\n",
    "usable_df = final_clean[usable_columns]\n",
    "\n",
    "# Export to CSV\n",
    "usable_df.to_csv(\"school_usable_columns.csv\", index=False)\n",
    "print(\"Exported: school_usable_columns.csv\")\n",
    "\n",
    "usable_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e481640-55d3-456c-917a-94fd9aa6c1db",
   "metadata": {},
   "source": [
    "# Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070717f8-0e7d-4f30-b89b-e8e6c4705a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Recreate the same train/test split for fairness\n",
    "X_train_svm, X_test_svm, y_train_svm, y_test_svm = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale X\n",
    "scaler_svm = StandardScaler()\n",
    "X_train_svm_scaled = scaler_svm.fit_transform(X_train_svm)\n",
    "X_test_svm_scaled = scaler_svm.transform(X_test_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b71ffeb-c6db-4d91-8c57-3cb803643757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "\n",
    "# probability = True enables predict_proba\n",
    "svm_linear = SVC(kernel=\"linear\", probability=True, random_state=42)\n",
    "\n",
    "svm_linear.fit(X_train_svm_scaled, y_train_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2a757b-0b6a-4700-8fbc-6aa1bd299764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class labels\n",
    "y_pred_svm = svm_linear.predict(X_test_svm_scaled)\n",
    "\n",
    "# Predict probabilities for log loss\n",
    "proba_svm = svm_linear.predict_proba(X_test_svm_scaled)\n",
    "\n",
    "print(\"Linear SVM Accuracy:\", accuracy_score(y_test_svm, y_pred_svm))\n",
    "print(\"Linear SVM Log-loss:\", log_loss(y_test_svm, proba_svm))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_svm, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a2d09b-198c-4800-8f93-457a177abc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "coef_df = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Coefficient\": svm_linear.coef_[0]\n",
    "}).sort_values(\"Coefficient\", ascending=False)\n",
    "\n",
    "coef_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fa0423-1bcd-4e8a-a105-2ca6751fd88e",
   "metadata": {},
   "source": [
    "trying non linear svm because reults are asymetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b03e85-db82-40d4-b7ab-5dd6e8a8816c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "\n",
    "# Same scaled data as linear SVM\n",
    "svm_rbf = SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\", probability=True, random_state=42)\n",
    "\n",
    "svm_rbf.fit(X_train_svm_scaled, y_train_svm)\n",
    "\n",
    "# Predictions\n",
    "y_pred_rbf = svm_rbf.predict(X_test_svm_scaled)\n",
    "proba_rbf = svm_rbf.predict_proba(X_test_svm_scaled)\n",
    "\n",
    "print(\"RBF SVM Accuracy:\", accuracy_score(y_test_svm, y_pred_rbf))\n",
    "print(\"RBF SVM Log-loss:\", log_loss(y_test_svm, proba_rbf))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test_svm, y_pred_rbf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7051784f-6486-46df-8679-0318f601c0c2",
   "metadata": {},
   "source": [
    "much better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4160f70-4b39-4335-996a-59afe0259ea9",
   "metadata": {},
   "source": [
    "Testing Naive Bayes (Gaussian) next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a40b84-2cdb-46e5-9543-9d4fb4a6731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train_svm_scaled, y_train_svm)\n",
    "\n",
    "y_pred_nb = nb.predict(X_test_svm_scaled)\n",
    "proba_nb = nb.predict_proba(X_test_svm_scaled)\n",
    "\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_score(y_test_svm, y_pred_nb))\n",
    "print(\"Naive Bayes Log-loss:\", log_loss(y_test_svm, proba_nb))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test_svm, y_pred_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb307af-77e9-46b0-9277-16e087be2089",
   "metadata": {},
   "source": [
    "not worth trying to fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f046529-5390-4011-9352-18db8d5f67cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=13)\n",
    "\n",
    "knn.fit(X_train_svm_scaled, y_train_svm)\n",
    "\n",
    "y_pred_knn = knn.predict(X_test_svm_scaled)\n",
    "proba_knn = knn.predict_proba(X_test_svm_scaled)\n",
    "\n",
    "print(\"kNN Accuracy:\", accuracy_score(y_test_svm, y_pred_knn))\n",
    "print(\"kNN Log-loss:\", log_loss(y_test_svm, proba_knn))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test_svm, y_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1e5c4d-06e4-4b45-b045-bbafa90a4a6d",
   "metadata": {},
   "source": [
    "Gradient Boosting time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b332cf2-ade4-4a79-be9a-4dd8cd812a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "\n",
    "# Gradient Boosting works fine on scaled or unscaled data\n",
    "# reuse the same scaled features you used for SVM\n",
    "# X_train_svm_scaled, X_test_svm_scaled, y_train_svm, y_test_svm\n",
    "\n",
    "gb = GradientBoostingClassifier(\n",
    "    random_state=42,\n",
    "    n_estimators=100,   \n",
    "    learning_rate=0.1,  \n",
    "    max_depth=3         \n",
    ")\n",
    "\n",
    "gb.fit(X_train_svm_scaled, y_train_svm)\n",
    "\n",
    "y_pred_gb = gb.predict(X_test_svm_scaled)\n",
    "proba_gb = gb.predict_proba(X_test_svm_scaled)\n",
    "\n",
    "print(\"Gradient Boosting Accuracy:\", accuracy_score(y_test_svm, y_pred_gb))\n",
    "print(\"Gradient Boosting Log-loss:\", log_loss(y_test_svm, proba_gb))\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test_svm, y_pred_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e38a16-1768-415f-8141-55f0c925143c",
   "metadata": {},
   "source": [
    "one last check just to see if restrictions can help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5c6edd-4705-4306-a111-4921b02cd7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduced feature set using school-level data + grade summaries\n",
    "reduced_features = [\n",
    "    \"ZIP\",\n",
    "    \"Budget (incl c/o) FY24\",\n",
    "    \"Expenditures FY24\",\n",
    "    \"% Exp FY24\",\n",
    "    \"Participation Rate (%)\",\n",
    "]\n",
    "\n",
    "# Work from the cleaned school-level dataset\n",
    "reduced_df = final_clean[reduced_features + [\"Pct_Met_Above\"]].copy()\n",
    "\n",
    "# Ensure all relevant columns are numeric\n",
    "for col in reduced_features + [\"Pct_Met_Above\"]:\n",
    "    reduced_df[col] = pd.to_numeric(reduced_df[col], errors=\"coerce\")\n",
    "\n",
    "# Drop rows with missing data (so X and y line up perfectly)\n",
    "reduced_df = reduced_df.dropna(subset=reduced_features + [\"Pct_Met_Above\"])\n",
    "\n",
    "# Features\n",
    "X_reduced = reduced_df[reduced_features].copy()\n",
    "\n",
    "# Target: High vs Low performance based on median Pct_Met_Above\n",
    "threshold_red = reduced_df[\"Pct_Met_Above\"].median()\n",
    "y_reduced = (reduced_df[\"Pct_Met_Above\"] >= threshold_red).astype(int)\n",
    "\n",
    "print(\"X_reduced shape:\", X_reduced.shape)\n",
    "print(\"y_reduced shape:\", y_reduced.shape)\n",
    "print(\"\\ny_reduced value counts:\\n\", y_reduced.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66edb89d-7683-4d9f-8ecb-fb252784eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train_red, X_test_red, y_train_red, y_test_red = train_test_split(\n",
    "    X_reduced, y_reduced, test_size=0.25, random_state=42, stratify=y_reduced\n",
    ")\n",
    "\n",
    "scaler_red = StandardScaler()\n",
    "X_train_red_s = scaler_red.fit_transform(X_train_red)\n",
    "X_test_red_s = scaler_red.transform(X_test_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a493f9-fc50-443c-806c-1b6d992cb67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "\n",
    "svm_rbf_red = SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\", probability=True, random_state=42)\n",
    "svm_rbf_red.fit(X_train_red_s, y_train_red)\n",
    "\n",
    "y_pred_rbf_red = svm_rbf_red.predict(X_test_red_s)\n",
    "proba_rbf_red = svm_rbf_red.predict_proba(X_test_red_s)\n",
    "\n",
    "print(\"RBF SVM (Reduced Features) Accuracy:\", accuracy_score(y_test_red, y_pred_rbf_red))\n",
    "print(\"RBF SVM (Reduced) Log-loss:\", log_loss(y_test_red, proba_rbf_red))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test_red, y_pred_rbf_red))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344296a6-f6ef-48b2-900c-632f34a19f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, log_loss,\n",
    "    precision_score, recall_score, f1_score\n",
    ")\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Set up data \n",
    "# Use your cleaned modeling dataset\n",
    "model_df = final_clean.copy()\n",
    "\n",
    "# Restricted feature set based on importance graph\n",
    "feature_cols = [\n",
    "    \"ZIP\",\n",
    "    \"Budget (incl c/o) FY24\",\n",
    "    \"Expenditures FY24\",\n",
    "    \"% Exp FY24\",\n",
    "    \"Participation Rate (%)\",\n",
    "]\n",
    "\n",
    "# Target variable \n",
    "threshold = model_df[\"Pct_Met_Above\"].median()\n",
    "y = (model_df[\"Pct_Met_Above\"] >= threshold).astype(int).values\n",
    "\n",
    "# Select features\n",
    "X = model_df[feature_cols].copy()\n",
    "\n",
    "# Ensure all are numeric\n",
    "for col in feature_cols:\n",
    "    X[col] = pd.to_numeric(X[col], errors=\"coerce\")\n",
    "\n",
    "# Drop rows with any missing values\n",
    "valid_mask = X.notna().all(axis=1)\n",
    "X = X[valid_mask]\n",
    "y = y[valid_mask.values]\n",
    "\n",
    "# Convert X to NumPy array\n",
    "X = X.values\n",
    "\n",
    "print(\"Final restricted dataset shape:\", X.shape, y.shape)\n",
    "\n",
    "# 2. Train/Val/Test split\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.20, random_state=42, stratify=y_train_val\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# Train a standard Random Forest\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    max_depth=None\n",
    ")\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Standard RF predictions\n",
    "proba_std = rf.predict_proba(X_test)\n",
    "y_pred_std = np.argmax(proba_std, axis=1)\n",
    "log_std = log_loss(y_test, proba_std)\n",
    "\n",
    "print(\"\\n=== Standard Random Forest (Restricted Features) ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_std))\n",
    "print(\"Log-loss:\", log_std)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_std))\n",
    "\n",
    "\n",
    "# Compute per-tree validation metrics\n",
    "tree_metrics = []\n",
    "val_proba_per_tree = []\n",
    "test_proba_per_tree = []\n",
    "\n",
    "for tree in rf.estimators_:\n",
    "    preds_val = tree.predict(X_val)\n",
    "\n",
    "    acc = accuracy_score(y_val, preds_val)\n",
    "    prec = precision_score(y_val, preds_val, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_val, preds_val, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_val, preds_val, average='weighted', zero_division=0)\n",
    "\n",
    "    tree_metrics.append([acc, prec, rec, f1])\n",
    "\n",
    "    val_proba_per_tree.append(tree.predict_proba(X_val))\n",
    "    test_proba_per_tree.append(tree.predict_proba(X_test))\n",
    "\n",
    "metrics_matrix = np.array(tree_metrics)\n",
    "val_proba_per_tree = np.array(val_proba_per_tree)\n",
    "test_proba_per_tree = np.array(test_proba_per_tree)\n",
    "\n",
    "print(\"\\nPer-tree metrics matrix shape:\", metrics_matrix.shape)\n",
    "\n",
    "# Helper function: evaluate weight vector on validation set\n",
    "def evaluate_weight_vector(w, metrics_matrix, val_proba_per_tree, y_val, temperature=1.0):\n",
    "    w = np.array(w, dtype=float)\n",
    "    composite_scores = metrics_matrix.dot(w)\n",
    "    tree_weights = softmax(composite_scores / temperature)\n",
    "\n",
    "    # Weighted probabilities for validation set\n",
    "    val_proba_weighted = np.tensordot(tree_weights, val_proba_per_tree, axes=(0, 0))\n",
    "    \n",
    "    val_ll = log_loss(y_val, val_proba_weighted)\n",
    "    val_pred = np.argmax(val_proba_weighted, axis=1)\n",
    "    val_acc = accuracy_score(y_val, val_pred)\n",
    "    \n",
    "    return val_ll, val_acc, tree_weights\n",
    "\n",
    "# Grid search over weight combinations\n",
    "temperature = 1.0\n",
    "candidate_vals = [0.0, 1.0, 2.0, 3.0]\n",
    "weight_grid = list(product(candidate_vals, repeat=4))  # 4^4 = 256 combos\n",
    "\n",
    "best = {\n",
    "    \"w\": None,\n",
    "    \"val_logloss\": np.inf,\n",
    "    \"val_acc\": 0.0,\n",
    "    \"weights_per_tree\": None\n",
    "}\n",
    "\n",
    "print(f\"\\nEvaluating {len(weight_grid)} weight combinations...\")\n",
    "\n",
    "for w in weight_grid:\n",
    "    ll, acc, tw = evaluate_weight_vector(\n",
    "        w, metrics_matrix, val_proba_per_tree, y_val, temperature\n",
    "    )\n",
    "\n",
    "    if ll < best[\"val_logloss\"]:\n",
    "        best[\"w\"] = w\n",
    "        best[\"val_logloss\"] = ll\n",
    "        best[\"val_acc\"] = acc\n",
    "        best[\"weights_per_tree\"] = tw\n",
    "\n",
    "print(\"\\nBest weight vector [acc, prec, rec, f1]:\", best[\"w\"])\n",
    "print(\"Best validation log-loss:\", round(best[\"val_logloss\"], 4))\n",
    "print(\"Best validation accuracy:\", round(best[\"val_acc\"], 4))\n",
    "\n",
    "# Evaluate tuned Softmax RF on the TEST set\n",
    "best_tree_weights = best[\"weights_per_tree\"]\n",
    "\n",
    "# Weighted probabilities for test set\n",
    "test_proba_weighted = np.tensordot(best_tree_weights, test_proba_per_tree, axes=(0, 0))\n",
    "test_logloss = log_loss(y_test, test_proba_weighted)\n",
    "y_test_pred = np.argmax(test_proba_weighted, axis=1)\n",
    "\n",
    "print(\"\\n=== Softmax RF with TUNED weights (Restricted Features, Test set) ===\")\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(\"Test Log-loss:\", test_logloss)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
